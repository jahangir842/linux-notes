****************************************************************
Apache Spark is an open-source, distributed computing system designed for fast processing of large-scale data. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Here’s an overview of its key components and capabilities:
****************************************************************
Key Components of Apache Spark:
****************************************************************
1. Spark Core: The foundation of Spark, providing basic I/O functionalities, task scheduling, and memory management.
2. Spark SQL: Module for structured data processing. It provides a programming interface for working with structured data using SQL.
3. Spark Streaming: Enables processing of live data streams.
4. MLlib: Spark’s machine learning library.
5. GraphX: API for graphs and graph-parallel computation.

*****************************************************************
list of commands for PySpark
*****************************************************************
------------------------------------------------------------------------------------------------------
| Command                               | Function                                                   |
|---------------------------------------|------------------------------------------------------------|
| `pyspark`                             | Starts the PySpark shell, allowing interactive Spark usage.|
| `spark-submit <Python/Spark script>`  | Submits a Spark application for execution.                 |
| `spark-shell`                         | Starts the Spark shell for Scala-based Spark development.   |
| `spark-sql`                           | Starts the Spark SQL CLI for executing SQL commands.       |
| `spark-class <class>`                 | Runs a Java/Scala class within the Spark environment.      |
| `spark-submit --master <master_url> ...` | Submits a Spark application to a specific master URL.     |
| `spark-submit --packages <package_name>` | Submits a Spark application with specified packages.      |
| `spark-submit --jars <jar_file>`      | Submits a Spark application with additional JAR files.     |
| `spark-submit --help`                 | Displays help information for using `spark-submit`.         |

****************************************************************
Key Terminologies related to Apache Spark:
****************************************************************
1. SparkContext:
    - The main entry point for Spark functionality. It is responsible for coordinating the execution of tasks over a cluster of machines and managing resources. SparkContext is used to create RDDs and connect to Spark cluster managers.
****************************************************************
Key Functions of SparkContext:

Connecting to a Cluster:
SparkContext allows you to connect to a Spark cluster, which can be a standalone cluster, Mesos, YARN, or Kubernetes.

Creating RDDs:
RDDs are the fundamental data structure in Spark. SparkContext provides methods to create RDDs from various data sources like local files, HDFS, HBase, Cassandra, and more.

Configuration:
You can configure various parameters like the number of cores, memory allocation, and other settings when initializing SparkContext.

Managing Jobs:
SparkContext is responsible for managing the job execution, including task scheduling and fault recovery.

****************************************************************
How to Use SparkContext
****************************************************************
Here's an example of python script:
****************************************************************
from pyspark import SparkContext, SparkConf

# Configure Spark
conf = SparkConf().setAppName("MyClusterApp").setMaster("spark://master:7077")
sc = SparkContext(conf=conf)

# Create an RDD from a file on HDFS
rdd = sc.textFile("hdfs://master:9000/user/hadoop/input.txt")

# Perform transformations and actions
result = rdd.flatMap(lambda line: line.split()).countByValue()

print(result)

# Stop the SparkContext
sc.stop()

****************************************************************
Key Methods of SparkContext

parallelize(collection): Distributes a local Python collection to form an RDD.
textFile(path): Reads a text file from HDFS, a local file system, or any Hadoop-supported file system URI.
setLogLevel(logLevel): Sets the log level (e.g., "INFO", "DEBUG", "ERROR").
stop(): Stops the SparkContext, which terminates the application.  
****************************************************************
2. RDD (Resilient Distributed Dataset):
    - The fundamental data structure of Spark, representing a distributed collection of elements that can be processed in parallel. RDDs are immutable and fault-tolerant, meaning they can be recomputed if lost.
****************************************************************
3. Transformation:
    - Operations that create a new RDD from an existing one. Transformations are lazy, meaning they are not executed immediately but are instead recorded as a lineage graph to be executed when an action is called. Examples include `map()`, `flatMap()`, `filter()`, and `reduceByKey()`.
****************************************************************
4. Action:
    - Operations that trigger the execution of transformations to return a result to the driver program or write data to an external storage system. Actions include `collect()`, `count()`, `saveAsTextFile()`, and `take()`.
****************************************************************
5. DAG (Directed Acyclic Graph):
    - A graphical representation of the sequence of transformations on RDDs that Spark constructs to track the lineage of computations. The DAG ensures the immutability of RDDs and fault tolerance.
****************************************************************
6. Partition:
    - A logical division of an RDD, representing a chunk of data that can be processed independently. Partitions enable parallel processing in Spark, as each partition can be processed on a different node in the cluster.
****************************************************************
7. Cluster Manager:
    - A system that manages resources (CPU, memory) and schedules tasks on a cluster of machines. Examples of cluster managers that Spark can work with include YARN, Mesos, and the standalone cluster manager.
****************************************************************
8. Executor:
    - A distributed agent responsible for executing tasks on worker nodes in the cluster. Each executor runs tasks in its own process and stores data for RDDs that are cached.
****************************************************************
9. Driver Program:
    - The main program that runs on the client machine and orchestrates the execution of Spark jobs. It creates the SparkContext and submits jobs to the cluster.
****************************************************************
10. Task:
    - A unit of work sent to an executor. Each task processes a partition of an RDD. Tasks are the smallest units of work in Spark.
****************************************************************
11. Job:
    - A sequence of transformations triggered by an action. Each job is divided into stages and tasks. A job corresponds to a single action call in the driver program.
****************************************************************
12. Stage:
    - A group of tasks that can be executed in parallel. Stages are determined by shuffle operations, which require data to be redistributed across partitions.
****************************************************************
13. Shuffle:
    - The process of redistributing data across partitions to perform operations such as `reduceByKey` or `groupByKey`. Shuffling involves network and disk I/O, making it one of the more expensive operations in Spark.
****************************************************************
14. Lazy Evaluation:
    - A feature of Spark where transformations are not executed immediately but are recorded in a lineage graph. The actual execution of transformations is deferred until an action is called, optimizing the execution plan.
****************************************************************
15. Broadcast Variable:
    - A read-only variable cached on each machine rather than shipping a copy of it with tasks. Broadcast variables are used to efficiently distribute large values (like lookup tables) to executors.
****************************************************************
16. Accumulator:
    - A variable that is used for aggregating information across tasks. Accumulators are used for implementing counters and sums. Only the driver program can read the value of the accumulator, but tasks can add to it.
****************************************************************

Example Usage of Some Terms: (for explaination)

****************************************************************
#:Example: Using RDDs, Transformations, Actions, and SparkContext
****************************************************************
from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext(master="local", appName="ExampleApp")

# Local collection
data = [1, 2, 3, 4, 5]

# Create an RDD
rdd = sc.parallelize(data)

# Perform a transformation (map) and an action (collect)
squared_rdd = rdd.map(lambda x: x * x)
result = squared_rdd.collect()

print(result)  # Output: [1, 4, 9, 16, 25]

# Stop the SparkContext
sc.stop()
****************************************************************

- **SparkContext: `sc` is the SparkContext, the main entry point for Spark.
- **RDD: `rdd` is an RDD created from the local collection `data`.
- **Transformation: `map` is a transformation applied to `rdd`.
- **Action: `collect` is an action that triggers the execution of the transformation and collects the results.

Understanding these key terminologies is essential for effectively working with Apache Spark and building efficient data processing pipelines.





****************************************************************
Deployment Modes: client mode and cluster mode. 
****************************************************************
### Client Mode
****************************************************************
In client mode, the Spark Driver runs on the machine from which the Spark application is submitted (often a user's local machine or an edge node in a cluster). The Spark Executors run on the worker nodes in the cluster.

Key Characteristics of Client Mode:
1. Driver Location: The driver runs on the client machine (the machine where the job is submitted).

2. Network Communication: The driver must communicate with the executors, requiring network connectivity between the client machine and the worker nodes.

3. Use Case: Suitable for interactive and development purposes where the user needs quick feedback and is typically running the application from their local machine.

4. Resource Management: The client machine must have enough resources (CPU, memory) to run the driver process, as the driver is responsible for maintaining information about the application’s structure and scheduling tasks.

Example:

      spark-submit --master yarn --deploy-mode client my_spark_app.py

****************************************************************
### Cluster Mode
****************************************************************
In cluster mode, the Spark Driver runs inside the cluster. When the application is submitted, the cluster manager (e.g., YARN, Kubernetes, Mesos) launches the driver inside the cluster.
****************************************************************
**Key Characteristics of Cluster Mode:**
1. **Driver Location:** The driver runs on a worker node inside the cluster, not on the client machine.
2. **Network Communication:** Since the driver runs in the cluster, it has a better network proximity to the executors, which can reduce communication latency.
3. **Use Case:** Suitable for production jobs and batch processing where the application needs to run reliably without user interaction and the client machine does not need to remain connected.
4. **Resource Management:** The cluster handles the resources for both the driver and the executors, ensuring better resource utilization and management.
****************************************************************
Example:

spark-submit --master yarn --deploy-mode cluster my_spark_app.py

****************************************************************
### Key Differences

1. **Driver Location:**
   - **Client Mode:** Runs on the machine from which the job is submitted.
   - **Cluster Mode:** Runs on a worker node within the cluster.

2. **Network Requirements:**
   - **Client Mode:** Requires network connectivity between the client machine and the cluster.
   - **Cluster Mode:** Network communication is internal to the cluster.

3. **Use Cases:**
   - **Client Mode:** Ideal for interactive and development purposes.
   - **Cluster Mode:** Ideal for production and long-running batch jobs.

4. **Resource Management:**
   - **Client Mode:** Client machine must be resourceful enough to handle the driver process.
   - **Cluster Mode:** Cluster resources are used to run both driver and executors.
****************************************************************
### Conclusion

Choosing between client mode and cluster mode depends on the specific requirements of your Spark application. For development and interactive use cases, client mode offers a convenient way to run and test applications. For production environments, where stability and resource management are crucial, cluster mode is typically preferred.

****************************************************************
	Execution Modes Modes  (Standalone and Local)
****************************************************************
Stand-alone Mode: Spark can be run in stand-alone mode, where it uses its own built-in cluster manager. In this mode, Spark does not require Hadoop's components such as HDFS or YARN. It manages its own resources and scheduling. The --master parameter while submitting application should specify the Spark master URL.
    
Spark Standalone Cluster consists of one or more master nodes and multiple worker nodes. The master node manages the cluster and schedules tasks, while worker nodes execute the tasks.
    
	spark-submit --master spark://master-ip:7077 --executor-memory 2G --total-executor-cores 4 your_spark_application.py

In this command:

    --master spark://master-ip:7077 specifies the address of the Spark master node.
    --executor-memory 2G sets the memory allocated to each executor to 2GB.
    --total-executor-cores 4 specifies the total number of CPU cores across all executors.
****************************************************************
    Local Mode: For development and testing purposes, Spark can also run in local mode on a single machine without the need for a Hadoop cluster. This is particularly useful for getting started with Spark, experimenting with code, and running small-scale jobs.
    
    spark-submit --master local[2] --executor-memory 1G your_spark_application.py

In this command:

    --master local[2] specifies that the application should run in local mode with 2 CPU cores.
    --executor-memory 1G sets the executor's memory to 1GB.



***************************************************************
Apache Spark Tools and Commands:
****************************************************************
1. spark-submit
****************************************************************
   Purpose Used to submit Spark applications to a cluster for execution.
****************************************************************   
   Syntax Example
****************************************************************
     spark-submit \
       --class com.example.MySparkApp \
       --master spark://localhost:7077 \      	(or yarn, mesos etc)
       --deploy-mode client \			(client or cluster)
       --executor-memory 2g \
       --total-executor-cores 4 \
****************************************************************
2. Spark on Terminal: 
****************************************************************
(spark-shell for Scala, pyspark for Python)
****************************************************************
   Purpose Interactive shell for running Spark code snippets and testing. The Spark Shell is an interactive environment provided by Apache Spark that allows you to quickly prototype, test, and execute Spark commands in a REPL (Read-Eval-Print Loop) environment. It is especially useful for data exploration and iterative development, as it provides immediate feedback on your code.
****************************************************************   
   Usage
     - Scala: `spark-shell`
     - Python: `pyspark`
****************************************************************
To start the PySpark shell, open your terminal and type:
	pyspark
****************************************************************
This command initializes the PySpark environment and opens an interactive shell where you can start writing Spark commands in Python.
****************************************************************
it automatically creates a SparkSession and a SparkContext for you:
****************************************************************
7. Spark History Server
****************************************************************
   Purpose Retains information about completed Spark applications and allows viewing historical data and logs.
   Access Typically available at `http://<history-server>:18080`.
****************************************************************
Cluster Managers 
****************************************************************
Spark can run on various cluster managers like 
	
	Spark's standalone cluster manager (spark's own)
	Apache Hadoop YARN
	Apache Mesos
	
The `--master` parameter in `spark-submit` specifies the cluster manager.
****************************************************************
Resource Allocation Use 
	--executor-memory 
	--total-executor-cores 
in `spark-submit` to allocate resources to Spark executors.
****************************************************************
Environment Setup Set `SPARK_HOME`, update `PATH`, and configure other environment variables for seamless Spark usage.
****************************************************************
Integration Spark integrates with other big data tools and ecosystems, such as 
	Apache Hadoop
	Apache Hive 
	Apache HBase, etc.
****************************************************************
Apache Spark and Apache Hadoop
****************************************************************
While Spark originally started as a project in the Hadoop ecosystem and can integrate well with 

	Hadoop's distributed file system (HDFS) and 
	resource manager (YARN)
	
it does not have a strict dependency on Hadoop. Here are a few points to consider:
****************************************************************
Compatibility with Other Storage Systems: 
****************************************************************
While Spark can work seamlessly with Hadoop's HDFS, it's also compatible with following storage systems such as 
	Hadoop's HDFS
	Amazon S3, 
	Azure Blob Storage,
	Google Cloud Storage 
and various databases through connectors and libraries.
	Apache Cassandra 
	Apache HBase 
****************************************************************
