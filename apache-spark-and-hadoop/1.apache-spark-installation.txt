****************************************************************
        		Apache Spark (a processing engine)
****************************************************************
How to Use Apache Spark:
****************************************************************
#1. Setup and Installation:
To start using Apache Spark, you need to set up a Spark cluster or run it locally on your machine. Here are the steps for a local setup:
****************************************************************
Install Java:
****************************************************************
for Ubuntu:
sudo apt install default-jdk
for RHEL:
sudo yum install java-devel

****************************************************************
Download and Install Apache Spark:
****************************************************************
  wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
  tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
 
  sudo mv spark-3.1.2-bin-hadoop3.2 /opt/spark
  sudo chmod -R 777 /opt/spark

****************************************************************  
Update bashrc: 
****************************************************************
echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$JAVA_HOME/bin'" >> ~/.bashrc
source ~/.bashrc

****************************************************************
Configure Spark
****************************************************************
Edit the `conf/spark-env.sh` file on the master node. If it doesnâ€™t exist, create it from the template: run the following commands;
****************************************************************
	cp /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh
	nano /opt/spark/conf/spark-env.sh
****************************************************************
And add the following configuration:
****************************************************************

export SPARK_MASTER_HOST='master-node-ip'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
****************************************************************

Create a `workers` file in the `conf` directory on the master node and add the IP addresses of your worker nodes:  run the following commands;
****************************************************************
cp /opt/spark/conf/worker.template /opt/spark/conf/worker
echo 'worker-node-1-ip' >> /opt/spark/conf/workers
echo 'worker-node-2-ip' >> /opt/spark/conf/workers
echo 'worker-node-3-ip' >> /opt/spark/conf/workers

****************************************************************
Start the Spark in Terminal:
****************************************************************
     - Scala: `spark-shell`
     - Python: `pyspark`
     
To view the Spark Web user interface:

http://<IP ADDRESS>:4040

The page shows your Spark URL, status information for workers, hardware resource utilization, etc.   ****************************************************************    
Start Standalone Spark Master Server
****************************************************************
In the terminal, type:
     start-master.sh
http://<IP ADDRESS>:8080
****************************************************************





****************************************************************
Additionaly Install Python:
****************************************************************
Install Python 3 Pip: If pip (Python package installer) is not already installed, you can install it using 
	sudo apt install python3-pip

Install PySpark with Pip:
	pip3 install pyspark

Verify Installation:
	pyspark
****************************************************************
By following above steps: We did 2 type of installations:
****************************************************************
    Core Spark Installation: The tar file provides the core Spark installation, including all necessary binaries and configuration files needed to run Spark.

    Python Integration: Installing PySpark with pip3 sets up the Python bindings and dependencies, making it possible to use Spark with Python and ensuring compatibility with other Python packages.

This dual approach ensures that your Spark installation is complete and that you can effectively use Spark within your Python environment.
****************************************************************
Generate SSH Key Pair on Master Node and send to all workers
****************************************************************
Passwordless SSH access is needed to the master node to allow seamless communication and task distribution between the master and worker nodes in a Spark cluster. It enables the master node to remotely execute commands and manage the worker nodes without manual password entry, facilitating automated cluster operations and job execution.
****************************************************************
	ssh-keygen -t rsa -b 4096 -C "your_email@example.com"

Ensure the SSH agent is running (optional but recommended for managing keys):

	eval "$(ssh-agent -s)"
****************************************************************
Add your private key to the SSH agent:

    ssh-add ~/.ssh/id_rsa
****************************************************************
Copy the Public Key to Worker Nodes

    ssh-copy-id user@worker-node-1-ip
    ssh-copy-id user@worker-node-2-ip
    ssh-copy-id user@worker-node-3-ip

Replace user with your actual username on the worker nodes.
****************************************************************
You should be able to log in to each worker node without being prompted for a password.
****************************************************************
